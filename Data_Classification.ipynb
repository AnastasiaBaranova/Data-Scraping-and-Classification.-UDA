{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "colab": {
      "name": "cp.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAQFgi3z1Ba1"
      },
      "source": [
        "# <center>Unstructured Data Analysis</center>"
      ]
    },
    {
      "source": [
        "MASNA 2019 Students <br>\n",
        "Baranova Anastasia <br>\n",
        "Gutman Irina"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uK01vHsz1Ba6",
        "outputId": "af2d2881-abc5-4ab9-eaa9-c6eb229459e8"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from sklearn.datasets import load_files\n",
        "nltk.download('stopwords')\n",
        "import pickle\n",
        "import pymorphy2\n",
        "from nltk.corpus import stopwords\n",
        "morphA = pymorphy2.MorphAnalyzer()\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from nltk.corpus import stopwords as nltk_stopwords\n",
        "stopwords = set(nltk_stopwords.words('russian') )"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /home/an/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to /home/an/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQE7NOLV1Ba6"
      },
      "source": [
        "Before classification we need to read the data scraped previously and take two columns from it: annotation and book_class (labels)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "-c3WJPUr1Ba7",
        "outputId": "79a5e59e-c2d5-4527-eb23-ab67e676b96d"
      },
      "source": [
        "data=pd.read_csv('data_for_classification.csv')\n",
        "data"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Unnamed: 0                                     genre  \\\n",
              "0                0               ['sf_social', 'sf_history']   \n",
              "1                1                         ['magician_book']   \n",
              "2                2                         ['magician_book']   \n",
              "3                3                 ['sf_space', 'narrative']   \n",
              "4                4                 ['sf_space', 'narrative']   \n",
              "...            ...                                       ...   \n",
              "101096      116606             ['det_action', 'adv_history']   \n",
              "101097      116607                              ['ref_dict']   \n",
              "101098      116608                            ['geo_guides']   \n",
              "101099      116609     ['child_education', 'sci_linguistic']   \n",
              "101100      116610  ['children', 'ref_encyc', 'sci_history']   \n",
              "\n",
              "                                               annotation  \\\n",
              "0       Мир на пороховой бочке, и несколько раз за век...   \n",
              "1       В довольно-таки мрачном фэнтезийном мире зарож...   \n",
              "2       Ты можешь летать на птице или нырять на оседла...   \n",
              "3       История одного батальона, отслеженная писателе...   \n",
              "4       Долгие годы последствия Галактической войны бу...   \n",
              "...                                                   ...   \n",
              "101096  В основу романа положен реальный, хотя и окруж...   \n",
              "101097  Предлагаемый немецко-русский русско-немецкий м...   \n",
              "101098  Сочи всегда был, есть и остается главным курор...   \n",
              "101099  Вслед за книгой «Рисую узоры» данное пособие п...   \n",
              "101100  Огнестрельное оружие давно и прочно стало леге...   \n",
              "\n",
              "                                                    title book_class  \n",
              "0                                             Армагед-дом      other  \n",
              "1                                                   Скрут      other  \n",
              "2                                                   Варан      other  \n",
              "3                                       Десант на Счастье      other  \n",
              "4                                   Повторная колонизация      other  \n",
              "...                                                   ...        ...  \n",
              "101096                      Секретный рейд адмирала Брэда      other  \n",
              "101097  Немецко-русский, русско-немецкий мини-словарь ...      other  \n",
              "101098                                   Олимпийский Сочи      other  \n",
              "101099            Пишу буквы. Для одаренных детей 5-6 лет    science  \n",
              "101100  Огнестрельное оружие. Иллюстрированный путевод...    science  \n",
              "\n",
              "[101101 rows x 5 columns]"
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>genre</th>\n      <th>annotation</th>\n      <th>title</th>\n      <th>book_class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>['sf_social', 'sf_history']</td>\n      <td>Мир на пороховой бочке, и несколько раз за век...</td>\n      <td>Армагед-дом</td>\n      <td>other</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>['magician_book']</td>\n      <td>В довольно-таки мрачном фэнтезийном мире зарож...</td>\n      <td>Скрут</td>\n      <td>other</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>['magician_book']</td>\n      <td>Ты можешь летать на птице или нырять на оседла...</td>\n      <td>Варан</td>\n      <td>other</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>['sf_space', 'narrative']</td>\n      <td>История одного батальона, отслеженная писателе...</td>\n      <td>Десант на Счастье</td>\n      <td>other</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>['sf_space', 'narrative']</td>\n      <td>Долгие годы последствия Галактической войны бу...</td>\n      <td>Повторная колонизация</td>\n      <td>other</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>101096</th>\n      <td>116606</td>\n      <td>['det_action', 'adv_history']</td>\n      <td>В основу романа положен реальный, хотя и окруж...</td>\n      <td>Секретный рейд адмирала Брэда</td>\n      <td>other</td>\n    </tr>\n    <tr>\n      <th>101097</th>\n      <td>116607</td>\n      <td>['ref_dict']</td>\n      <td>Предлагаемый немецко-русский русско-немецкий м...</td>\n      <td>Немецко-русский, русско-немецкий мини-словарь ...</td>\n      <td>other</td>\n    </tr>\n    <tr>\n      <th>101098</th>\n      <td>116608</td>\n      <td>['geo_guides']</td>\n      <td>Сочи всегда был, есть и остается главным курор...</td>\n      <td>Олимпийский Сочи</td>\n      <td>other</td>\n    </tr>\n    <tr>\n      <th>101099</th>\n      <td>116609</td>\n      <td>['child_education', 'sci_linguistic']</td>\n      <td>Вслед за книгой «Рисую узоры» данное пособие п...</td>\n      <td>Пишу буквы. Для одаренных детей 5-6 лет</td>\n      <td>science</td>\n    </tr>\n    <tr>\n      <th>101100</th>\n      <td>116610</td>\n      <td>['children', 'ref_encyc', 'sci_history']</td>\n      <td>Огнестрельное оружие давно и прочно стало леге...</td>\n      <td>Огнестрельное оружие. Иллюстрированный путевод...</td>\n      <td>science</td>\n    </tr>\n  </tbody>\n</table>\n<p>101101 rows × 5 columns</p>\n</div>"
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw17T89_-55I"
      },
      "source": [
        "d = data[['annotation', 'book_class']]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPXAdPGWA2Be"
      },
      "source": [
        "X, y = d.annotation, d.book_class"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "source": [
        "After assigning annotations and labels, we need to normalize our texts."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjNWz1a41Ba8"
      },
      "source": [
        "documents = []\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "stemmer = WordNetLemmatizer()\n",
        "\n",
        "for sen in range(0, len(X)):\n",
        "    # Remove all the special characters\n",
        "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
        "    \n",
        "    # remove all single characters\n",
        "    document = re.sub(r'\\s+[а-яА-Яa-zA-Z]\\s+', ' ', document)\n",
        "    \n",
        "    # Remove single characters from the start\n",
        "    document = re.sub(r'\\^[а-яА-Яa-zA-Z]\\s+', ' ', document)\n",
        "    \n",
        "    # Substituting multiple spaces with single space\n",
        "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "    \n",
        "    # Removing prefixed 'b'\n",
        "    document = re.sub(r'^b\\s+', '', document)\n",
        "    \n",
        "    # Converting to Lowercase\n",
        "    document = document.lower()\n",
        "    \n",
        "    # Lemmatization\n",
        "    document = document.split()\n",
        "    document = [morphA.parse(word)[0].normal_form for word in document] \n",
        "\n",
        "    document = ' '.join(document)\n",
        "    documents.append(document)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "source": [
        "Next we vectorize the normalized annotations and create Bag of Words."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "\n",
        "vectorizer = CountVectorizer(max_features=1500, stop_words=stopwords)\n",
        "X = vectorizer.fit_transform(documents).toarray()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "h1pxBvgP1Ba8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "Next we use TF-IDF in order to take into account the frequencies of words."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTHPJ8m01Ba8"
      },
      "source": [
        "\n",
        "tfidfconverter = TfidfTransformer()\n",
        "X = tfidfconverter.fit_transform(X).toarray()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "source": [
        "Then we split our data into train and test data (80/20)."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qux8gfvu1Ba9"
      },
      "source": [
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "source": [
        "Next we decided to use different classifiers for our purpose of creating a model which can predict the genre/topic of a book by its annotation."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "### Logistic Regression"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train, y_train)\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6QzFyzt1Ba9",
        "outputId": "0d285b28-0a72-4404-bbef-2155b3125fe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bk3wN31KEX27"
      },
      "source": [
        "y_pred = classifier.predict(X_test)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKOcPaEa1Ba9",
        "outputId": "03cbebfe-7514-4783-d3f8-7b9cbd70ee7c"
      },
      "source": [
        "score = classifier.score(X_test, y_test)\n",
        "\n",
        "print(\"Accuracy: \", score)\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.866030364472578\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       other       0.89      0.93      0.91     14581\n",
            "     science       0.80      0.70      0.74      5640\n",
            "\n",
            "    accuracy                           0.87     20221\n",
            "   macro avg       0.84      0.81      0.83     20221\n",
            "weighted avg       0.86      0.87      0.86     20221\n",
            "\n"
          ]
        }
      ]
    },
    {
      "source": [
        "The accuracy is 0.86, which is quite good."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtZ3c-UBEh3y"
      },
      "source": [
        "#Saving the model\n",
        "with open('text_classifier', 'wb') as picklefile:\n",
        "    pickle.dump(classifier,picklefile)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "source": [
        "### Random Forest"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(n_estimators=1000, random_state=0)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
        "classifier.fit(X_train, y_train) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[14045   536]\n",
            " [ 1761  3879]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       other       0.89      0.96      0.92     14581\n",
            "     science       0.88      0.69      0.77      5640\n",
            "\n",
            "    accuracy                           0.89     20221\n",
            "   macro avg       0.88      0.83      0.85     20221\n",
            "weighted avg       0.89      0.89      0.88     20221\n",
            "\n",
            "0.8864052222936551\n"
          ]
        }
      ],
      "source": [
        "y_pred = classifier.predict(X_test)\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "source": [
        "Random Forest took more time but it shows a slightly better result: 0.88."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "### Linear Classifiers"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/an/venv/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:574: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\"Maximum number of iteration reached before \"\n",
            "Accuracy:  0.8632609663221403\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       other       0.88      0.93      0.91     14581\n",
            "     science       0.80      0.68      0.74      5640\n",
            "\n",
            "    accuracy                           0.86     20221\n",
            "   macro avg       0.84      0.81      0.82     20221\n",
            "weighted avg       0.86      0.86      0.86     20221\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "classifier = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5)\n",
        "classifier.fit(X_train, y_train)\n",
        "score = classifier.score(X_test, y_test)\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "print(\"Accuracy: \", score)\n",
        "print(classification_report(y_test,y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.8653874684733692\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       other       0.88      0.94      0.91     14581\n",
            "     science       0.81      0.67      0.74      5640\n",
            "\n",
            "    accuracy                           0.87     20221\n",
            "   macro avg       0.85      0.81      0.82     20221\n",
            "weighted avg       0.86      0.87      0.86     20221\n",
            "\n"
          ]
        }
      ],
      "source": [
        "classifier = SGDClassifier(loss=\"modified_huber\", penalty=\"l2\", max_iter=50)\n",
        "classifier.fit(X_train, y_train)\n",
        "score = classifier.score(X_test, y_test)\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "print(\"Accuracy: \", score)\n",
        "print(classification_report(y_test,y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.8670688887789921\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       other       0.90      0.91      0.91     14581\n",
            "     science       0.77      0.75      0.76      5640\n",
            "\n",
            "    accuracy                           0.87     20221\n",
            "   macro avg       0.84      0.83      0.83     20221\n",
            "weighted avg       0.87      0.87      0.87     20221\n",
            "\n"
          ]
        }
      ],
      "source": [
        "classifier = SGDClassifier(loss=\"modified_huber\", penalty=\"l2\", max_iter=50)\n",
        "classifier.fit(X_train, y_train)\n",
        "score = classifier.score(X_test, y_test)\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "print(\"Accuracy: \", score)\n",
        "print(classification_report(y_test,y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.8523317343355917\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       other       0.86      0.95      0.90     14581\n",
            "     science       0.82      0.60      0.69      5640\n",
            "\n",
            "    accuracy                           0.85     20221\n",
            "   macro avg       0.84      0.78      0.80     20221\n",
            "weighted avg       0.85      0.85      0.84     20221\n",
            "\n"
          ]
        }
      ],
      "source": [
        "classifier = SGDClassifier(loss=\"log\", penalty=\"elasticnet\", max_iter=50)\n",
        "classifier.fit(X_train, y_train)\n",
        "score = classifier.score(X_test, y_test)\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "print(\"Accuracy: \", score)\n",
        "print(classification_report(y_test,y_pred))"
      ]
    },
    {
      "source": [
        "### CatBoost"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30 70 86.617872508778 77.53238126868149 72.90755777638975 82.78368794326241\n",
            "35 65 87.47836407694972 78.08930425752855 76.26774847870182 80.0\n",
            "40 60 87.64156075367192 77.66955589312842 78.29219960367502 77.05673758865248\n",
            "45 55 87.7355224766332 77.01149425287355 80.69153069153069 73.65248226950355\n",
            "50 50 87.84432026111469 76.41527537900595 83.27059807611877 70.60283687943263\n",
            "55 45 87.27065921566688 74.38805970149255 84.76190476190476 66.27659574468086\n",
            "35 65\n",
            "[[13177  1404]\n",
            " [ 1128  4512]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "n_list=[]\n",
        "k_list=[]\n",
        "F_value_list=[]\n",
        "conf_mat =[]\n",
        "\n",
        "# Finding best parametrs of the model with the weights. Weights are needed since we have unbalanced classes.\n",
        "for n in range (30,60,5): #was found by experimental knowledge\n",
        "    n_list.append(n)\n",
        "    #\n",
        "    k=100-n\n",
        "    k_list.append(k)\n",
        "    #\n",
        "    CatBoost_model = CatBoostClassifier(iterations=250,class_weights=[n,k])\n",
        "    CatBoost_model.fit(X_train,y_train,silent = True)\n",
        "    y_pred_lc = CatBoost_model.predict(X_test)\n",
        "    cm_CatBoost_model = confusion_matrix(y_test,y_pred_lc)\n",
        "    conf_mat.append(cm_CatBoost_model)\n",
        "    #\n",
        "    numerator = cm_CatBoost_model[1][1]+cm_CatBoost_model[0][0] #True Posotive + False Negative\n",
        "    denominator =  cm_CatBoost_model[1][1] + cm_CatBoost_model[0][1] + cm_CatBoost_model[1][0] + cm_CatBoost_model[0][0] # All\n",
        "    accuracy = (numerator/denominator) * 100\n",
        "    #\n",
        "    numerator = cm_CatBoost_model[1][1] #True Posotive\n",
        "    denominator =  cm_CatBoost_model[1][1] +  cm_CatBoost_model[0][1] # True Positive + False Positive\n",
        "    prec = (numerator/denominator) * 100\n",
        "    #\n",
        "    numerator = cm_CatBoost_model[1][1] #True Posotive\n",
        "    denominator =  cm_CatBoost_model[1][1] +  cm_CatBoost_model[1][0] # True Positive + False Negative\n",
        "    rec = (numerator/denominator) * 100\n",
        "    #\n",
        "    F_value = 2*prec*rec/(prec+rec) # we can add a little bit of magic right here and the best n and k would change\n",
        "    F_value_list.append(F_value)\n",
        "    #\n",
        "    print (n,k,accuracy,F_value, prec,rec)\n",
        "    \n",
        "best_n = n_list[F_value_list.index(max(F_value_list))]\n",
        "best_k = k_list[F_value_list.index(max(F_value_list))]\n",
        "best_conf_mat = conf_mat[F_value_list.index(max(F_value_list))]\n",
        "print (best_n, best_k)\n",
        "print (best_conf_mat)"
      ]
    },
    {
      "source": [
        "Overall, all models showed good result (accuracy ≈0.86-0.88)"
      ],
      "cell_type": "markdown",
      "metadata": {}
    }
  ]
}